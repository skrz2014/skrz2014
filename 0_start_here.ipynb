{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "# :chart_with_upwards_trend: Telco Churn Model - Part 1 (Data Loading and Exploration)\n",
    "\n",
    "### Please run this first Notebook fully before running the second Notebook.\n",
    "\n",
    "### First, add the `imbalanced-learn`, `snowflake-ml-python`, `altair`, `pandas`, and `numpy` packages from the package picker on the top right. We will be using these packages later in the notebook.\n",
    "\n",
    "In this solution, we will play the role of a data scientist at a telecom company that wants to identify users who are at high risk of churning. To accomplish this, we need to build a model that can learn how to identify such users. We will demonstrate how to use Snowflake Notebook in conjunction with Snowflake/Snowpark to build a Random Forest Classifier to help us with this task.\n",
    "\n",
    "### Prerequisites\n",
    "- Familiarity with basic Python and SQL\n",
    "- Familiarity with training ML models\n",
    "- Familiarity with data science notebooks\n",
    "- Go to the Snowflake sign-up page and register for a free account. After registration, you will receive an email containing a link that will take you to Snowflake, where you can sign in.\n",
    "\n",
    "### What You'll Learn\n",
    "- How to import/load data with Snowflake Notebook\n",
    "- How to train a Random Forest with Snowpark ML model\n",
    "- How to visualize the predicted results from the forecasting model\n",
    "- How to build an interactive web app and make predictions on new users\n",
    "\n",
    "## Importing Data\n",
    "To pull our churn dataset into SnowSight notebooks, we will pull some parquet data from AWS S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f6327-5d70-4f06-afee-a969e855ced5",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE STAGE TELCO_CHURN_EXTERNAL_STAGE_DEMO\n",
    "    URL = 's3://sfquickstarts/notebook_demos/churn/';\n",
    "\n",
    "CREATE FILE FORMAT IF NOT EXISTS MY_PARQUET_FORMAT TYPE = PARQUET COMPRESSION = SNAPPY;\n",
    "\n",
    "CREATE TABLE if not exists TELCO_CHURN_RAW_DATA_DEMO USING TEMPLATE ( \n",
    "    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*)) \n",
    "    FROM \n",
    "        TABLE( INFER_SCHEMA( \n",
    "        LOCATION => '@TELCO_CHURN_EXTERNAL_STAGE_DEMO', \n",
    "        FILE_FORMAT => 'MY_PARQUET_FORMAT',\n",
    "        FILES => 'telco_churn.parquet'\n",
    "        ) \n",
    "    ) \n",
    ");\n",
    "\n",
    "COPY INTO TELCO_CHURN_RAW_DATA_DEMO\n",
    "FROM @TELCO_CHURN_EXTERNAL_STAGE_DEMO\n",
    "FILES = ('telco_churn.parquet')\n",
    "FILE_FORMAT = (\n",
    "    TYPE=PARQUET,\n",
    "    REPLACE_INVALID_CHARACTERS=TRUE,\n",
    "    BINARY_AS_TEXT=FALSE\n",
    ")\n",
    "MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n",
    "ON_ERROR=ABORT_STATEMENT;\n",
    "\n",
    "SELECT * FROM TELCO_CHURN_RAW_DATA_DEMO;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ebdf6-dab3-4096-aec8-26c9fe17f603",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "# Working with Data\n",
    "\n",
    "We can start working with the data using our familiar data science libraries in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71421f3-e57c-47fe-8a6d-505e139391b7",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import altair as alt\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Getting session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
    "                     \"name\":\"churn_prediction\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0},\n",
    "                     \"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\"}}\n",
    "\n",
    "telco_churn_snow_df = cell2.to_df()\n",
    "telco_churn_snow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181caa90-d298-4763-998b-38a04764bb6a",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell5"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Machine learning models thrive on clean and well-organized data. To ensure our models perform at their best, we'll investigate our dataset to address any missing values and visualize the distributions of each column.\n",
    "\n",
    "### Basic Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5fa2b7-fb41-411f-af93-5b8990a1cd7f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "telco_churn_snow_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9282527-2b63-4b6a-9c7b-085525fd9367",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "### Checking nulls with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ed78d-c98d-4119-8f20-8924b90c4b67",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "telco_churn_pdf = telco_churn_snow_df.to_pandas()\n",
    "session.create_dataframe(telco_churn_pdf).write.save_as_table(\"telco_churn_pdf\", mode=\"overwrite\")\n",
    "telco_churn_pdf.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e641dee-1fa7-428c-a04f-88019983436d",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "As can be seen, there is no null value in any of the feature columns.\n",
    "\n",
    "### Visualizing Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78bb87-047e-4706-9268-0ec01a26bb93",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "columns = telco_churn_pdf.columns\n",
    "num_columns_for_display = 3\n",
    "col1, col2 , col3 = st.columns(num_columns_for_display)\n",
    "index = 0\n",
    "for col in columns:\n",
    "    source = pd.DataFrame(telco_churn_pdf[col])\n",
    "    chrt = alt.Chart(source).mark_bar().encode(\n",
    "    alt.X(f\"{col}:Q\", bin=True),\n",
    "    y='count()',\n",
    "    )\n",
    "    if index % num_columns_for_display == 0:\n",
    "        with col1: \n",
    "            st.altair_chart(chrt)\n",
    "    elif index % num_columns_for_display == 1:\n",
    "        with col2: \n",
    "            st.altair_chart(chrt)\n",
    "    elif index % num_columns_for_display == 2:\n",
    "        with col3: \n",
    "            st.altair_chart(chrt)\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126aef0-a085-4525-8ce7-c976e21332b3",
   "metadata": {
    "collapsed": false,
    "name": "cell11"
   },
   "source": [
    "### Understanding Churn Rate - Imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d1847-a0d9-4f39-bf24-4d340db5b891",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "telco_churn_snow_df.group_by('\"Churn\"').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8eeb31-8a03-435d-a3dd-67d210db0fe0",
   "metadata": {
    "collapsed": false,
    "name": "cell13"
   },
   "source": [
    "If you want to understand a model, you need to know its weaknesses. When the target variable has one class that is much more frequent than the other, your data is imbalanced. This causes issues when evaluating models since both classes don't get equal attention.\n",
    "\n",
    "In contrast to modeling an imbalanced dataset, a model trained on balanced data sees an equal amount of observations per class. By eliminating the imbalance, we also eliminate the model's potential to achieve high metric scores due to bias towards a majority class. This means that when we evaluate our model, the metrics can capture a better representation of how well the model does at making valuable predictions.\n",
    "\n",
    "#### Comparing Big data processing with pandas v.s. Snowpark Dataframes\n",
    "\n",
    "For the groupby aggregation query above, we used Snowpark dataframes to perform the operation. Snowpark's Dataframe API allows you to query and process data at scale in Snowflake. With Snowpark, you no longer have to convert your dataframes to pandas in memory. Snowpark lets process data in Snowflake without moving data to the system where your application code runs, and process at scale as part of the elastic and serverless Snowflake engine.\n",
    "\n",
    "Below we look at how the query performance of the groupby aggregation with Snowpark v.s. pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27268649-5b9c-42d8-b5df-fa8d442d869f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "telco_churn_snow_df.group_by('\"Churn\"').count()\n",
    "end = time.time()\n",
    "st.markdown(f\"Total Time with Snowpark: {end-start}\")\n",
    "\n",
    "start = time.time()\n",
    "telco_churn_snow_pdf = telco_churn_snow_df.to_pandas()\n",
    "end_mid = time.time()\n",
    "telco_churn_snow_pdf.groupby(\"Churn\").count()\n",
    "end = time.time()\n",
    "st.markdown(f\"Total Time with Pandas: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294550ee-81df-418a-89f7-343eb86de87b",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "We can see that Snowpark runs much faster. This is because of the I/O overhead for converting a Snowpark dataframe to pandas. We can see that the bulk of the time spent is on I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975a94e-d2cf-4988-a186-bf671755cf5d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "st.markdown(f\"I/O time to convert to Pandas dataframe: {end_mid-start}\")\n",
    "st.markdown(f\"Processing time with Pandas dataframe: {end-end_mid}\")\n",
    "st.markdown(f\"I/O account for {(end_mid-start)/(end-start)*100:.2f}% of processing time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17948f8c-1904-4fc8-84b3-8430cd22cd85",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "### This concludes the first Notebook. Please go to the second Notebook to continue this solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
